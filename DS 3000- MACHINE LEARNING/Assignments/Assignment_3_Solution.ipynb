{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aKr4SC0U-kn"
   },
   "source": [
    "# Assignment 3: Classification with Logistic Regression\n",
    "\n",
    "# Total: /100\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Complete the assignment\n",
    "\n",
    "* Once the notebook is complete, restart your kernel and rerun your cells\n",
    "\n",
    "* Submit this notebook to owl by the deadline\n",
    "\n",
    "* You may use any python library functions you wish to complete the assignment.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The goal of our classifier is to predict, using a logistic regression, if a patient may take a certain drug.\n",
    "\n",
    "The dataset contains both numerical and categorical input variables, while the response variable ('Drug') has multiple levels. To simplify our analysis here, we focus on predicting if a patient may take \"Drug-Y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYk5_SCPU-kw"
   },
   "outputs": [],
   "source": [
    "# You may need these\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "seed=0\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axyFmaF8U-kz"
   },
   "source": [
    "## Question 1: /18 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm0M4J0WU-k3"
   },
   "source": [
    "1. Read in the `drug.csv` dataset and display the first 5 rows.\n",
    "2. Print out all columns in the dataset and list categorical variables. Use the build-in function `get_dummies()` to convert all categorical variables (**exclude Drug variable**) to dummy variables. You may read the official explanations for more information on `get_dummies()` [here](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). What's the size of the dataframe after transforming it?\n",
    "3. Map the target values from yes/no to 1/0. What is the baseline accuracy for this classification problem? Round into 1 decimal place (for example, 50.1% or 0.501)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cNADOoXU-k6"
   },
   "source": [
    "1.1 Read the dataset and display the first 8 rows, and print out all columns in the dataset and **list** all categorical variables in the answer part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "wAV0h1zoU-k8",
    "outputId": "5227d95a-b3ce-49f1-c214-33fc9018b23e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>25.355</td>\n",
       "      <td>DrugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>13.093</td>\n",
       "      <td>DrugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>10.114</td>\n",
       "      <td>DrugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>7.798</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>18.043</td>\n",
       "      <td>DrugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>8.607</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>16.275</td>\n",
       "      <td>DrugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>11.037</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Sex      BP Cholesterol  Na_to_K   Drug\n",
       "0   23    F    HIGH        HIGH   25.355  DrugY\n",
       "1   47    M     LOW        HIGH   13.093  DrugY\n",
       "2   47    M     LOW        HIGH   10.114  DrugY\n",
       "3   28    F  NORMAL        HIGH    7.798  drugX\n",
       "4   61    F     LOW        HIGH   18.043  DrugY\n",
       "5   22    F  NORMAL        HIGH    8.607  drugX\n",
       "6   49    F  NORMAL        HIGH   16.275  DrugY\n",
       "7   41  NaN     LOW        HIGH   11.037  drugC"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1.1 code here\n",
    "df = pd.read_csv('drug.csv')\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDfdxrWDpauF",
    "outputId": "d65676e4-313f-4f61-a713-0aab1acb2360"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K', 'Drug'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gzTrEzt1pb2A",
    "outputId": "9397c622-7bd6-4427-e52f-28eb3b489350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age              int64\n",
       "Sex             object\n",
       "BP              object\n",
       "Cholesterol     object\n",
       "Na_to_K        float64\n",
       "Drug            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "# 5 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yL9tqfndpRbI"
   },
   "source": [
    "**YOUR ANSWER HERE:** [1pt] *Categorical columns: \"Sex\", \"BP\",\"Cholesterol\" and \"Drug\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2W98Z97mfxI"
   },
   "source": [
    "1.2 Check that is any there missing value in each column of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLp-EpaFnIYm",
    "outputId": "523f8342-9ad7-42f5-de1e-c72c7f54585c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age            False\n",
       "Sex             True\n",
       "BP             False\n",
       "Cholesterol    False\n",
       "Na_to_K        False\n",
       "Drug           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1.2 code here\n",
    "df.isnull().any()\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTIk_tnonYFR"
   },
   "source": [
    "1.3 Replace all the missing values in **Sex** to be 'M'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6s1do3KoOmU"
   },
   "outputs": [],
   "source": [
    "# Question 1.3 code here\n",
    "df= df.fillna('M')\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UXEH2aKU-k-"
   },
   "source": [
    "1.4 Use the build-in function `get_dummies()` to convert all categorical variables (**excluding `Drug` variable**) to dummy variables. What's the size of the dataframe after transforming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-dsj9mrU-lA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Question 1.4 code here\n",
    "df = pd.get_dummies(df,columns=[\"Sex\", \"BP\",\"Cholesterol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "biHH3GYiU-lH",
    "outputId": "5ed3b9d5-9df7-4387-da3a-26deecf81025"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krlMrqnIU-lK"
   },
   "source": [
    "**YOUR ANSWER HERE:** [1pt] *The size is (200,10)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbsurWVJq7IY"
   },
   "source": [
    "1.5 Transform all the labels that is 'DrugY' in **`Drug`** to be value 1, otherwise, to be 0. Then transform the type of **`Drug`** to be 'int'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeNDEYoxrkZr"
   },
   "outputs": [],
   "source": [
    "# Question 1.5 code here\n",
    "df.loc[-df[\"Drug\"].isin(['DrugY']),\"Drug\"]=\"0\"\n",
    "df.loc[df[\"Drug\"].isin(['DrugY']),\"Drug\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGPnoEB_8hZu"
   },
   "outputs": [],
   "source": [
    "df.Drug=df.Drug.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSFggWr19pcv",
    "outputId": "20b5e1cd-5bc6-434c-c32b-a22db830f4d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                     int64\n",
       "Na_to_K               float64\n",
       "Drug                    int64\n",
       "Sex_F                   uint8\n",
       "Sex_M                   uint8\n",
       "BP_HIGH                 uint8\n",
       "BP_LOW                  uint8\n",
       "BP_NORMAL               uint8\n",
       "Cholesterol_HIGH        uint8\n",
       "Cholesterol_NORMAL      uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LORXe5nU-lL"
   },
   "source": [
    "1.6. What is the baseline accuracy for this classification problem? Round into 1 decimal place (for example, 0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tDl3SaGU-lL",
    "outputId": "4f6cffe1-7a92-4b9d-e230-b470e6af0067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    102\n",
       "0     98\n",
       "Name: Drug, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "df.Drug.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOs6n1qJU-lN",
    "outputId": "55679a77-fa52-41c6-ba1c-f424ef2e1815"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(102/(98+102),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mgFJFbaYU-lP",
    "outputId": "af8eca0c-d695-49e3-d4d7-e6efaf960292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or \n",
    "counts= df.Drug.value_counts()\n",
    "baseline_accuracy = round(counts[1]/(counts[0]+counts[1]), 1)\n",
    "baseline_accuracy\n",
    "# 2 pts (keep 1 decimal in both the result of codes and answer part below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpiZmxAtU-lQ"
   },
   "source": [
    "**YOUR ANSWER HERE:** [1pt] *the baseline accuracy is 0.5 (or 50.0%)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svT8YoBjU-lR"
   },
   "source": [
    "## Question 2: /7 pts\n",
    "\n",
    "Split the data into train and test for the outcome/response and the predictor variables. Hold out 25% of observations as the test set.  Pass `random_state=11` to `train_test_split` to ensure you get the same train and tests sets as the solution. Your dependent variable in the the dataset is named as `Drug`. How many patients who take \"DrugY\" in the train dataset and test dataset, respectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZabWhfKqU-lS"
   },
   "outputs": [],
   "source": [
    "# Question 2 code here.\n",
    "\n",
    "# Don't use the \"Drug\" as a feature\n",
    "X = df.drop('Drug', axis='columns').values\n",
    "\n",
    "y = df.Drug.values\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X,y,test_size=0.25,random_state=11)\n",
    "# 5 pts correct code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-KTEBRk5MuH",
    "outputId": "d700befc-ffff-40b7-8d1e-633f71509698"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFFSep1W3cfs",
    "outputId": "7aed3b20-d5a2-43d3-817c-1d7eaf11819d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ytest)\n",
    "# 1pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04g5eqFb4UHe"
   },
   "source": [
    "**YOUR ANSWER HERE:** [1pt] *In the train dataset, there are 76 patients who take 'DrugY', and 26 in the test dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-NwTJR2U-lU"
   },
   "source": [
    "## Question 3: /20 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrQkrgnKU-lV"
   },
   "source": [
    "3.1 Create a instance of sklearn's `LogisticRegression` object for **unpenalized** logistic regression.\n",
    "Using this object, **run a logisitic regression analysis** of `Drug` (y-variable) against `Age` and `Na_to_K` (x-variables) using your training data. Report the parameters (variables and intercept) of your model as indicated with the `print` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "upgKwmOMU-lV",
    "outputId": "89b2aad5-5497-432c-cb04-b067953b0a98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01090483,  0.65752731]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3 code here\n",
    "LOGREG_first_two = LogisticRegression(solver='lbfgs', penalty='none', max_iter=10000)\n",
    "Xtrain_set1 = Xtrain[:,[0,1]]\n",
    "Xtest_set1 = Xtest[:,[0,1]]\n",
    "lr_first_two = LOGREG_first_two.fit(Xtrain_set1,ytrain)\n",
    "lr_first_two.coef_\n",
    "# 8 pts note they are required to use unpenalized regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spMlCZt4U-lW",
    "outputId": "0035c066-3cc1-4560-e466-8f759e703ec4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters of the model asociated to age and Na_to_K are respectively: [-0.01090483  0.65752731]\n"
     ]
    }
   ],
   "source": [
    "print(f'The parameters of the model asociated to age and Na_to_K are respectively: {lr_first_two.coef_[0]}')\n",
    "# 1pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0PvF8h4U-lX",
    "outputId": "89f0dcf1-e4ca-4b35-8ea4-b7ac96ed4f41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intercept of the model is -8.720013063429404\n"
     ]
    }
   ],
   "source": [
    "print(f'The intercept of the model is {lr_first_two.intercept_[0]}')\n",
    "# 1 pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kheT1mJBU-lY"
   },
   "source": [
    "3.2 Compute 4 label-based criteria, namely, 'Accuracy', 'Precision', 'Sensitivity' and 'Specificity' for your two variables only classifier **using the test data** (Round into 4 decimal place). Use a threshold of 0.5. Answer the questions in this text box below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeCznyblU-la"
   },
   "outputs": [],
   "source": [
    "## Put code here to compute criteria:\n",
    "def compute_performance(yhat, y, classes):\n",
    "    # First, get tp, tn, fp, fn\n",
    "    tp = sum(np.logical_and(yhat == classes[1], y == classes[1]))\n",
    "    tn = sum(np.logical_and(yhat == classes[0], y == classes[0]))\n",
    "    fp = sum(np.logical_and(yhat == classes[1], y == classes[0]))\n",
    "    fn = sum(np.logical_and(yhat == classes[0], y == classes[1]))\n",
    "\n",
    "    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # Precision\n",
    "    # \"Of the ones I labeled +, how many are actually +?\"\n",
    "    precision = tp / (tp + fp)\n",
    "    \n",
    "    # Sensitivity\n",
    "    # \"Of all the + in the data, how many do I correctly label?\"\n",
    "    sensitivity = tp / (tp + fn) \n",
    "    \n",
    "    # Specificity\n",
    "    # \"Of all the - in the data, how many do I correctly label?\"\n",
    "    specificity = tn / (fp + tn)\n",
    "    \n",
    "    # Print results\n",
    "    \n",
    "    print(\"Accuracy:\",round(acc,4),\"Precision:\",round(precision,4),\n",
    "          \"Sensitivity:\",round(sensitivity,4),\"Specificity:\",round(specificity,4))\n",
    "    #5 pts, calculating or reporting extra \"Recall\" will lose 1 pt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVHdh8JPU-lb",
    "outputId": "673141b0-be89-4c46-b410-03f37838ee1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 25 tn: 22 fp: 2 fn: 1\n",
      "Accuracy: 0.94 Precision: 0.9259 Sensitivity: 0.9615 Specificity: 0.9167\n",
      "Value of amount for positive example: 27\n"
     ]
    }
   ],
   "source": [
    "## Put the code you need to answer the following questions.\n",
    "\n",
    "ytest_hat = LOGREG_first_two.predict(Xtest_set1)\n",
    "compute_performance(ytest_hat, ytest, LOGREG_first_two.classes_)\n",
    "\n",
    "## Put code here to compute whatever else you might need to answer the question.\n",
    "\n",
    "print(f\"Value of amount for positive example: {sum(ytest_hat > 0)}\")\n",
    "\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUGuT8L1U-lb"
   },
   "source": [
    "* How many of the test instances are labeled positive by your classifier?\n",
    "\n",
    "**YOUR ANSWER HERE:**[1 pt] *27 cases are classified as positive.*\n",
    "\n",
    "* Does this classifier reach the baseline accuracy?\n",
    "\n",
    "**YOUR ANSWER HERE:** [1 pt] *Yes, the accuracy for the model is 94% vs. the baseline is 50%.*\n",
    "\n",
    "* Is this classifier useful for classifying drug-Y? Explain in one or two sentences using the performance matrix results.\n",
    "\n",
    "**YOUR ANSWER HERE:**[1 pt] *Since the classifier has significantly higher accuracy than the baseline accuracy, and the precision and sensitivity are high, this classifier is useful.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9mq1cndaU-lc"
   },
   "source": [
    "## Question 4: /8 pts\n",
    "Now with default penalty method, fit two logistic regression models using tuning parameter $C=0.1$ and $C=1$ to the training data and include all the variables in the data frame (except for `Drug`) in the cell below. You will want to make new objects like you did for the simpler model. Print the parameters (variables and intercept) you obtain, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BiH4XfJHU-le",
    "outputId": "ad285ab0-fd9a-4ff2-f103-f66fc07831ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0130806 ,  0.60277064, -0.11304191,  0.11306467, -0.24194974,\n",
       "         0.36553947, -0.12356698,  0.23756636, -0.23754361]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for Question 4\n",
    "LOGREG_1 = LogisticRegression(solver='lbfgs',penalty = 'l2', max_iter=10000,C=0.1)\n",
    "lr_all_1 = LOGREG_1.fit(Xtrain,ytrain)\n",
    "lr_all_1.coef_\n",
    "#4 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ljd3dv8VU-le",
    "outputId": "0b2272e3-6379-40b5-e309-edbf36e74e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.95642709])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_all_1.intercept_\n",
    "#1pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E51-Zhl0mm67",
    "outputId": "e117a6d4-910d-4f81-eb3c-fc820e2665d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0251735 ,  0.77566215, -0.26073171,  0.26067561, -0.7210371 ,\n",
       "         1.18493053, -0.46394953,  0.66529231, -0.66534841]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGREG_2 = LogisticRegression(solver='lbfgs',penalty = 'l2', max_iter=10000,C=1)\n",
    "lr_all_2 = LOGREG_2.fit(Xtrain,ytrain)\n",
    "lr_all_2.coef_\n",
    "#1pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJmta-mam0g-",
    "outputId": "f3bda697-256d-4f17-b5fd-45ecbaa1ae73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.92561835])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_all_2.intercept_\n",
    "#1pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Jc2UNoYnGhD"
   },
   "source": [
    "* Describe the differences between the fitted model parameters obtained from the two models here.\n",
    "\n",
    "**YOUR ANSWER:** [1 pt] *Larger tuning parameter $C$ yields larger absolute value of model parameters, which means that larger tuning parameter $C$ corresponds to smaller strength of penalization.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMUsdjPfU-lh"
   },
   "source": [
    "## Question 5: /15 pts\n",
    "\n",
    "In the cell below, compute the 4 label-based criteria we mentioned in Question 3.2 for the two new classifiers using the test data. (You don't have to copy the function down into this cell; just call it again here.) Use a threshold of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWsf59jcU-li",
    "outputId": "b9d9dd3b-ffeb-437b-8f7f-bdedfb482242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 25 tn: 24 fp: 0 fn: 1\n",
      "Accuracy: 0.98 Precision: 1.0 Sensitivity: 0.9615 Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Code for  Code for answering questions in the below cell\n",
    "ytest_hat_all_1 = lr_all_1.predict(Xtest)\n",
    "compute_performance(ytest_hat_all_1, ytest, lr_all_1.classes_)\n",
    "# 5 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBXlY0YIde4e",
    "outputId": "0895c996-8cc8-4824-a064-619ec614939f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of amount for positive example: 25\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value of amount for positive example: {sum(ytest_hat_all_1 > 0)}\")\n",
    "#1pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMbVvqBiqQpr",
    "outputId": "116f2490-05b5-4f2e-f517-fed5c56d1782"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 24 tn: 24 fp: 0 fn: 2\n",
      "Accuracy: 0.96 Precision: 1.0 Sensitivity: 0.9231 Specificity: 1.0\n"
     ]
    }
   ],
   "source": [
    "ytest_hat_all_2 = lr_all_2.predict(Xtest)\n",
    "compute_performance(ytest_hat_all_2, ytest, lr_all_2.classes_)\n",
    "#2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUhQmZmhqRLa",
    "outputId": "6d9f0e76-e49c-4e0a-d904-58b4f2ed5aab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of amount for positive example: 24\n"
     ]
    }
   ],
   "source": [
    "print(f\"Value of amount for positive example: {sum(ytest_hat_all_2 > 0)}\")\n",
    "#1pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXXGvWDYU-lj"
   },
   "source": [
    "* How many of the test instances are respectively labeled positive by the two classifiers?\n",
    "\n",
    "**YOUR ANSWER:** [3 pts] *25 and 24 are respectively labeled positive.* \n",
    "\n",
    "* Among the classifiers in Questions 3 and 4, which one is the **best classifier** for classifying Drug? Explain the possible reason in one or two sentences.\n",
    "\n",
    "**YOUR ANSWER:** [3 pts] *The classifier in Question 4 with smaller tuning parameter $C$ performs best. This classifier contains more variables (i.e., more complex model), which helps to learn more. In addition, smaller tuning parameter $C$ gives stronger penalization which might help the classifier avoid overfitting problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtWayoFDU-lk"
   },
   "source": [
    "## Question 6: /14 pts\n",
    "In the cell below, predict the class coding your own sigmoid function (do NOT use  the predict function from sklear). Compare the first 5 rows using the three models constructed in question 4 and question 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zYNJn57U-ll"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "# 3pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Otwe4JGf51En",
    "outputId": "bd34fd22-9ba0-4e56-c66b-e49d0e6034c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31345361],\n",
       "       [0.99999017],\n",
       "       [0.95907755],\n",
       "       [0.04709426],\n",
       "       [0.99998092]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = np.dot(Xtest[:,[0,1]],LOGREG_first_two.coef_.T) + LOGREG_first_two.intercept_\n",
    "sigmoid(z1)[0:5]\n",
    "# 3pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I29VHeud51En",
    "outputId": "b421971b-2df0-4ee7-9ff3-e59a8dfc96b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23554322],\n",
       "       [0.99994536],\n",
       "       [0.94361639],\n",
       "       [0.0558983 ],\n",
       "       [0.99995873]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2 = np.dot(Xtest,lr_all_1.coef_.T) + lr_all_1.intercept_\n",
    "sigmoid(z2)[0:5]\n",
    "# 1pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1chaxAiKU-lm",
    "outputId": "ae9da9ba-ef35-448a-db66-7ad9e200dea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0991672 ],\n",
       "       [0.99999003],\n",
       "       [0.9788813 ],\n",
       "       [0.02097274],\n",
       "       [0.99999846]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z3 = np.dot(Xtest,lr_all_2.coef_.T) + lr_all_2.intercept_\n",
    "sigmoid(z3)[0:5]\n",
    "# 1pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HUOZPnzU-ln"
   },
   "source": [
    "* Is the probability given by the sigmoid function the probability of a case being negative?\n",
    "\n",
    "**YOUR ANSWER:**[3pts] *No, it is the probability of being labelled as positive.*\n",
    "\n",
    "* By just looking over the first 5 cases, how does the probabilities obtained from the two classifiers in Question 4 changes? Does the change of probability actually change the classification results for the first 5 cases?\n",
    "\n",
    "**YOUR ANSWER:**[3pts] *With larger $C$, those probabilities close to 1 further increase, whereas, those probabilities close to 0 further decrease. The changes does not really change the final results for the first 5 cases though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08R44Tq-U-lo"
   },
   "source": [
    "# Question 7: /7 pts\n",
    "\n",
    "Plot ROC curves for all of your classifiers using the cells below, then answer the following questions, computing whatever quantities you need to answer them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "IGol7oXQ51Eo",
    "outputId": "accb3e55-98e3-4eb8-d748-b0ab8a5d5230"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775641025641025"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "# ROC for the first classifier\n",
    "ytest_prob_amount = LOGREG_first_two.predict_proba(Xtest_set1)\n",
    "fpr, tpr, _ = roc_curve(ytest, ytest_prob_amount[:,1], pos_label=1)\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(fpr,tpr)\n",
    "ax.set(xlabel=\"FPR\",ylabel=\"TPR\")\n",
    "auc(fpr,tpr)\n",
    "#4 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "lAZGcLXy51Eo",
    "outputId": "6c0355ee-5d52-4e28-9b2c-b43badd8ac7d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823717948717949"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC for the second classifier\n",
    "ytest_prob_1 = lr_all_1.predict_proba(Xtest)\n",
    "fpr, tpr, _ = roc_curve(ytest, ytest_prob_1[:,1], pos_label=1)\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(fpr,tpr)\n",
    "ax.set(xlabel=\"FPR\",ylabel=\"TPR\")\n",
    "auc(fpr,tpr)\n",
    "#1 pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "m7O0aKt6U-lp",
    "outputId": "8cfb2c5d-e2f4-4659-dbf7-925a9a9110a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9903846153846154"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC for the third classifier\n",
    "ytest_prob_2 = lr_all_2.predict_proba(Xtest)\n",
    "fpr, tpr, _ = roc_curve(ytest, ytest_prob_2[:,1], pos_label=1)\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(fpr,tpr)\n",
    "ax.set(xlabel=\"FPR\",ylabel=\"TPR\")\n",
    "auc(fpr,tpr)\n",
    "#1 pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS6tI4eaU-lp"
   },
   "source": [
    "* Which classifier has a highest estimated probability of correctly distinguishing between a positive and a negative instance? How do you know?\n",
    "\n",
    "**YOUR ANSWER:** [1 pts] *The AUROC gives the probability that a + is correctly distinguished from a -. Therefore the third model (contains all variables and $C=1$) has the best performance since its AUROC is 0.990 which is the largest.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gfLLOYjkyfc"
   },
   "source": [
    "# Question 8: /11 pts\n",
    "\n",
    "Multiclass Logistic Regression\n",
    "\n",
    "In the classification lab, we trained a binary LR classifier using the _mnist_ dataset to discriminate entries which were equal to 5 from the rest. This time you have 10 classes i.e., 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. Use the same dataset and `SGDClassifier` to tain a multiclass logistic regression model with `l2` regularization. For training, include these arguments too: `max_iter=2000`, `tol=1e-3`, `random_state=seed`. For computation speed-up, some `sklearn` functions take the argument `n_jobs=N` to run in parallel. A good value for N can be the number of physical CPU cores that your machine possesses. Check the documentations of the functions to take advantage from this where applicable. For splitting the data use `test_size=0.5` and `random_state=seed`.\n",
    "\n",
    "Put your classifier and the `StandardScaler()` into a pipeline using `make_pipeline`. Therefore, your final model will be a pipeline that always standardizes the data before feeding it to the classifier. Use the `classification_report` to report the performance of your final model (*i.e.*, the pipeline) over the **test set**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t3tDHZTkyfc"
   },
   "outputs": [],
   "source": [
    "# We will be using the MNIST dataset, which is a set of 70000 small images of handwritten digits.\n",
    "# Each image is labeled with the digit it represents.\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "x_mnist = mnist[\"data\"]\n",
    "y_mnist = mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_mnist, y_mnist, test_size=0.5, random_state=seed)\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNqWI7dHkyfc"
   },
   "outputs": [],
   "source": [
    "# After reading the `SGDClassifier` documentation you realize the following argument settings:\n",
    "# loss='hinge' gives linear support-vector machine\n",
    "# loss='log` gives logistic regression which is what we want in this question.\n",
    "\n",
    "clf4 = make_pipeline(StandardScaler(), SGDClassifier(loss='log', penalty=\"l2\", max_iter=2000, tol=1e-3, n_jobs=4, random_state=seed)).fit(X_train, y_train)\n",
    "# 5 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTx_ZWOtkyfc",
    "outputId": "7559c09c-189e-4269-ec84-3a12f19349ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.94      0.95      3535\n",
      "           1       0.97      0.96      0.96      3954\n",
      "           2       0.91      0.89      0.90      3475\n",
      "           3       0.92      0.86      0.89      3546\n",
      "           4       0.92      0.92      0.92      3386\n",
      "           5       0.89      0.83      0.86      3158\n",
      "           6       0.94      0.94      0.94      3389\n",
      "           7       0.95      0.91      0.93      3652\n",
      "           8       0.73      0.92      0.81      3392\n",
      "           9       0.89      0.87      0.88      3513\n",
      "\n",
      "    accuracy                           0.90     35000\n",
      "   macro avg       0.91      0.90      0.90     35000\n",
      "weighted avg       0.91      0.90      0.91     35000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf4.predict(X_test)))\n",
    "# 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diSyV5aLkyfc"
   },
   "source": [
    "Let's see how the model generalizes to new data.\n",
    "\n",
    "You can run the cell below to see how well your model can recognize a digit written by the mouse cursor. Set the `final_model` variable according to the name choses for your pipeline, run the cell, draw on the pop-up canvas, and once you close the canvas you will see the result.\n",
    "\n",
    "This code will not work on headless servers such as Colab. You need to install the `tk-tools` package and run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8PiWFjxkyfd"
   },
   "outputs": [],
   "source": [
    "final_model =   # here use the name of your pipeline\n",
    "\n",
    "#!pip install tk-tools\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib as mpl\n",
    "\n",
    "temp_file_name=\"TEMP_image_TEMP.jpg\"\n",
    "\n",
    "app = Tk()\n",
    "app.geometry(\"300x300\")\n",
    "\n",
    "canvas = tk.Canvas(app, bg='white')\n",
    "canvas.pack(anchor='nw', fill='both', expand=1)\n",
    "\n",
    "def get_x_and_y(event):\n",
    "    global lasx, lasy\n",
    "    lasx, lasy = event.x, event.y\n",
    "\n",
    "def draw_smth(event):\n",
    "    global lasx, lasy\n",
    "    canvas.create_line((lasx, lasy, event.x, event.y), fill='red', width=3.5)\n",
    "    lasx, lasy = event.x, event.y\n",
    "    ps = canvas.postscript(colormode = 'color')\n",
    "    img = Image.open(io.BytesIO(ps.encode('utf-8')))\n",
    "    img.save(temp_file_name)\n",
    "\n",
    "canvas.bind(\"<Button-1>\", get_x_and_y)\n",
    "canvas.bind(\"<B1-Motion>\", draw_smth)\n",
    "\n",
    "app.mainloop()\n",
    "\n",
    "img = Image.open(temp_file_name)\n",
    "#resize image to 28x28 pixels\n",
    "img = img.resize((28,28))\n",
    "#convert rgb to grayscale\n",
    "img = img.convert(\"L\")\n",
    "img = np.array(img)\n",
    "img = 255.0 - img\n",
    "print(\"Your input:\")\n",
    "plt.imshow(img, cmap = mpl.cm.binary); plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# reshaping to support our model input\n",
    "img = np.reshape(img, 28*28)\n",
    "#predicting the class\n",
    "print('Input recognized as ' + str(final_model.predict([img])[0])+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14DrGa-tkyfd"
   },
   "source": [
    "* Despite showing great scores in training and testing stages, why your model does not generalize well to new data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn4Hxgtwkyfd"
   },
   "source": [
    "**YOUR ANSWER:** [2 pts]\n",
    "\n",
    "Because new data is not comming from the same data generating process. There is differences between the traning data and new data. Things such as location of the digit on the canvas, resolution etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
